from datetime import datetime
from sqlalchemy.orm import Session
from typing import List, Optional
import hashlib
import json

from app.models import article_model as models
from app.schemas import article_schema as schemas
from app.models import ai_analysis_model
from app.crud import ai_analysis_crud  
from app.services import gemini_service
from app.services.event_publisher import event_publisher
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_article_by_url(db: Session, url: str) -> Optional[models.Article]:
    """L·∫•y article theo URL"""
    return db.query(models.Article).filter(models.Article.url == url).first()

def get_article_by_content_hash(db: Session, content_hash: str) -> Optional[models.Article]:
    """L·∫•y article theo content hash"""
    return db.query(models.Article).filter(models.Article.content_hash == content_hash).first()

async def create_article(db: Session, article: schemas.ArticleCreate) -> models.Article:
    """T·∫°o article m·ªõi v√† publish event"""
    
    # T√≠nh content hash
    content_to_hash = (article.title or "") + (article.summary or "")
    content_hash = hashlib.md5(content_to_hash.encode('utf-8')).hexdigest()
    
    # Ki·ªÉm tra tr√πng l·∫∑p
    existing_article_by_url = get_article_by_url(db, url=article.url)
    if existing_article_by_url:
        logger.info(f"üìÑ Article ƒë√£ t·ªìn t·∫°i (URL): {article.title[:50]}...")
        return existing_article_by_url
    
    existing_article_by_hash = get_article_by_content_hash(db, content_hash=content_hash)
    if existing_article_by_hash:
        logger.info(f"üìÑ Article ƒë√£ t·ªìn t·∫°i (Content): {article.title[:50]}...")
        return existing_article_by_hash
    
    # T·∫°o article m·ªõi
    article_dict = article.dict()
    article_dict['content_hash'] = content_hash
    
    db_article = models.Article(**article_dict)
    db.add(db_article)
    db.commit()
    db.refresh(db_article)
    
    logger.info(f"‚úÖ T·∫°o article m·ªõi: {article.title[:50]}...")
    
    # **PH√ÇN T√çCH AI V·ªöI GEMINI**
    ai_analysis_data = None
    try:
        logger.info(f"ü§ñ ƒêang ph√¢n t√≠ch b√†i vi·∫øt b·∫±ng Gemini...")
        
        # 1. T√≥m t·∫Øt
        ai_summary = gemini_service.summarize_article_with_gemini(
            title=db_article.title,
            content=db_article.summary or ""
        )

        # 2. Ph√¢n t√≠ch to√†n di·ªán
        full_analysis = gemini_service.analyze_article_with_gemini(
            title=db_article.title,
            content=db_article.summary or ""
        )

        #T·∫°o r·ªóng data tr∆∞·ªõc
        db_ai_analysis = ai_analysis_model.ArticleAIAnalysis(
                article_id=db_article.id,
                summary="",                 # Chu·ªói r·ªóng thay v√¨ None
                category="Kh√¥ng r√µ",                   # Kh√¥ng c√≥ ph√¢n lo·∫°i
                sentiment_score=0.0,           # M·∫∑c ƒë·ªãnh trung l·∫≠p
                impact_score=0.0,              # M·∫∑c ƒë·ªãnh 0
                keywords_extracted="[]",       # JSON r·ªóng
                analysis_metadata="{}"         # Metadata r·ªóng
            )
        
        if ai_summary:
            db_ai_analysis = ai_analysis_model.ArticleAIAnalysis(
                article_id=db_article.id,
                summary=ai_summary or "",  # N·∫øu None th√¨ r·ªóng
            )

        if full_analysis:
            db_ai_analysis.category = full_analysis.get("category", "")
            sentiment_map = {"T√≠ch c·ª±c": 1.0, "Trung t√≠nh": 0.0, "Ti√™u c·ª±c": -1.0}
            db_ai_analysis.sentiment_score = sentiment_map.get(full_analysis.get("sentiment"), 0.0)
            impact_map = {"Cao": 1.0, "Trung b√¨nh": 0.5, "Th·∫•p": 0.1}
            db_ai_analysis.impact_score = impact_map.get(full_analysis.get("impact_level"), 0.0)
            db_ai_analysis.keywords_extracted = json.dumps(full_analysis.get("key_entities", []), ensure_ascii=False)
            db_ai_analysis.analysis_metadata = json.dumps(full_analysis, ensure_ascii=False)

        # 5. L∆∞u AI analysis
        db.add(db_ai_analysis)
        db.commit()
        db.refresh(db_ai_analysis)
        
        # Prepare data cho event
        ai_analysis_data = {
            "category": db_ai_analysis.category,
            "sentiment_score": db_ai_analysis.sentiment_score,
            "impact_score": db_ai_analysis.impact_score,
            "keywords": full_analysis.get("key_entities", []) if full_analysis else [],
            "analysis_summary": full_analysis.get("analysis_summary", "") if full_analysis else "",
            "sentiment_text": full_analysis.get("sentiment", "") if full_analysis else "",
            "impact_text": full_analysis.get("impact_level", "") if full_analysis else ""
        }
        
        logger.info(f"‚úÖ ƒê√£ l∆∞u AI analysis v·ªõi ID: {db_ai_analysis.id}")
        
    except Exception as e:
        logger.info(f"‚ö†Ô∏è L·ªói khi ph√¢n t√≠ch AI: {e}")
        ai_analysis_data = None
    
    # **PUBLISH EVENT THAY V√å DIRECT CALL**
    try:
        event_data = {
            "event_type": "article_created",
            "article_id": db_article.id,
            "title": db_article.title,
            "url": db_article.url,
            "summary": db_article.summary,
            "source_url": db_article.source_url,
            "created_at": db_article.created_at.isoformat(),
            "ai_analysis": ai_analysis_data,
            "timestamp": datetime.now().isoformat(),
            "service_name": "news_service"
        }
        
        # Publish event async
        await event_publisher.publish_article_created(event_data)
        logger.info(f"üì§ ƒê√£ publish event cho article: {db_article.title[:50]}...")
        
    except Exception as e:
        logger.info(f"‚ö†Ô∏è L·ªói khi publish event: {e}")
        # V·∫´n return article d√π publish event th·∫•t b·∫°i
    
    return db_article

def get_articles(db: Session, skip: int = 0, limit: int = 20) -> List[models.Article]:
    """L·∫•y danh s√°ch articles v·ªõi ph√¢n trang"""
    return db.query(models.Article)\
             .order_by(models.Article.created_at.desc())\
             .offset(skip)\
             .limit(limit)\
             .all()

def get_articles_count(db: Session) -> int:
    """ƒê·∫øm t·ªïng s·ªë articles"""
    return db.query(models.Article).count()

def get_articles_with_ai_analysis(db: Session, skip: int = 0, limit: int = 20):
    """L·∫•y articles k√®m AI analysis"""
    return ai_analysis_crud.get_articles_with_ai_analysis(db, skip, limit)

def get_articles_by_category(db: Session, category: str):
    """L·∫•y articles theo category AI"""
    return ai_analysis_crud.get_articles_by_category(db, category)

def get_high_impact_articles(db: Session, min_impact: float = 0.7):
    """L·∫•y articles c√≥ impact cao"""
    return ai_analysis_crud.get_high_impact_articles(db, min_impact)
