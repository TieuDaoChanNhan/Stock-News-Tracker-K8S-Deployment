import requests
from datetime import datetime
import asyncio
from app.database import SessionLocal
from app.crud import crawl_source_crud, article_crud
from app.services.generic_crawler import scrape_news_from_website
from app.schemas.article_schema import ArticleCreate

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def fetch_and_process_all_active_sources():
    """Fetch v√† process tin t·ª©c t·ª´ c√°c ngu·ªìn ƒëang ho·∫°t ƒë·ªông"""
    logger.info(f"\nüïê {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - B·∫Øt ƒë·∫ßu chu k·ª≥ crawl tin t·ª©c...")
    
    db = SessionLocal()
    total_articles = 0
    
    try:
        # L·∫•y danh s√°ch ngu·ªìn crawl active
        sources = crawl_source_crud.get_active_crawl_sources(db)
        logger.info(f"üìä T√¨m th·∫•y {len(sources)} ngu·ªìn ƒëang ho·∫°t ƒë·ªông.")
        
        for source in sources:
            try:
                logger.info(f"üîÑ Crawling: {source.name}")
                
                # Crawl articles t·ª´ ngu·ªìn
                articles_data = scrape_news_from_website(
                    page_url=source.url,
                    article_container_selector=source.article_container_selector,
                    title_selector=source.title_selector,
                    link_selector=source.link_selector,
                    summary_selector=source.summary_selector,
                    date_selector=source.date_selector,
                    source_name=source.name,
                    max_articles=5
                )
                
                # L∆∞u articles v√†o database
                for article_data in articles_data:
                    try:
                        article_create = ArticleCreate(
                            title=article_data['title'],
                            url=article_data['url'],
                            summary=article_data['summary'],
                            published_date_str=article_data['published_date_str'],
                            source_url=source.url
                        )
                        
                        # Async create article s·∫Ω t·ª± ƒë·ªông trigger AI analysis v√† publish event
                        await article_crud.create_article(db, article_create)
                        total_articles += 1
                        
                    except Exception as e:
                        logger.info(f"   ‚ùå L·ªói khi l∆∞u article: {e}")
                        continue
                
                # C·∫≠p nh·∫≠t th·ªùi gian crawl cu·ªëi
                crawl_source_crud.update_crawl_source_last_crawled_at(
                    db, source.id, datetime.now()
                )
                
            except Exception as e:
                logger.info(f"‚ùå L·ªói khi crawl {source.name}: {e}")
                continue
        
        logger.info(f"‚úÖ Ho√†n th√†nh chu k·ª≥ crawl: {total_articles} articles ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")
        
    finally:
        db.close()

def main():
    """Main function ƒë·ªÉ ch·∫°y m·ªôt l·∫ßn"""
    logger.info("üöÄ News Service Scheduler - Single Run")
    logger.info("=" * 60)
    
    # Ch·∫°y async function
    asyncio.run(fetch_and_process_all_active_sources())
    
    logger.info("‚úÖ News Service Scheduler completed!")

if __name__ == "__main__":
    main()
